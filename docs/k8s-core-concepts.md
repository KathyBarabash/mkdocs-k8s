---
title: Core Concepts
summary: 
authors:
    - Kathy Barabash
date: 2022-01-13
---

# Kubernetes Core Concepts

__k8s__ is a container orchestrator acting as a desired state controller, with API to define the desired state and the system to take the necessary actions to achieve that desired state.

## Kubernetes API

[k8s REST API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/) archtecture has changed over time. Initial API version, still available through `/api/v1`, was hard to extend. Current API architecture supports API groups and is extensible through __CRDs__ (CustomResourceDefinitions). Current k8s API methods belong to one of the API groups and can be classified to one of the three maturity groups: _Alpha_, _Beta_, and _Stable_.  API Clients such as `kubectl` exist to convert consize commands to k8s API REST calls. 

---

## Kubernetes Objects

[k8s Objects](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/) such as _pod_, _service_, _deployment_, etc. are persistent entities that represent both the desired and the actual state of the k8s system by containing two nested fields:

- The __spec__, provided by the end-user as YAML files
- The __status__, generated by the kubernetes control plane through monitoring and observation

There are three ways to [create k8s objects](https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/):

- Imperative commands, e.g. `kubectl run nginx –image nginx`
- Imperative object configuration, e.g. `kubectl [create/delete/replace] commands`
- Declarative object configuration, e.g. `kubectl apply `

[Object names](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/) have to be unique per object type per namespace but can be reused for different types and across namespaces. k8s generates an  historically unique UIDs for all the objects.
It is possible to obtain the UID of a resource through `kubectl get` command, e.g. `kubectl get pod captureorder-5d6fd597d4-jhdqf -o json | jq .metadata.uid`

---

## k8s Namespaces

[Namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) are used to create multiple ‘virtual clusters’ on top of a single kubernetes cluster.
Most `kubectl` commands are performed in a context of a default namespace. This can be changed by adding the `--namespace=xxx` flag. Commands addressing all the namespaces require the `--all-namespaces` flag. To see what is the default namespace, run `kubectl config view | grep namespace`. To set the default namespace, run `kubectl config set-context --current --namespace=xxx`.

Namespaces are also important in a context of DNS resolution. k8s creates DNS entry for every k8s service in a form of `service.namespace.svc.cluster.local` FQNA. To reach services outside of the current default namespace, one has to specify this FQDN rather than just the service name.

---

## k8s Labels, Selectors and Annotation

k8s supports [labels and selectors](http:// https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) to organize objects into sets for ease of reference. 
To create a service linked to a certain deployment, it is possible to label the deployment and specify this label as a selector in the service specification YAML, e.g.:

__Deployement spec__
``` 
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: azure-vote-front
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: azure-vote-front
    spec:
      containers:
      - name: azure-vote-front
        image: microsoft/azure-vote-front:redis-v1
        ports:
        - containerPort: 80
        env:
        - name: REDIS
          value: "azure-vote-back"
```

__Service spec__
```
apiVersion: v1
kind: Service
metadata:
  name: azure-vote-front
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: azure-vote-front
```
In the above example, we create a deployment named `azure-vote-front` with the label `app: azure-vote-front`. Afterwards, we create a service named `azure-vote-front` which references that deployment with the selector `app: azure-vote-front`.

Labels and selectors are not unique by design and multiple k8s resources can have the same label attached to them. Selector can specify labels that are equal (through `=` or `==`) or not equal (through `!=`). Selectors can also specify sets of labels, e.g. `app in (web, db, backend)` or `app not in (monitoring, backup)`.

In addition to labels and selectors, k8 supports assigning arbitrary metada to resources through [annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/). In the following example, annotation is used to create a service with an Azure load balancer of the internal type:
```
apiVersion: v1
kind: Service
metadata:
  name: internal-app
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: "true"
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: internal-app
```

---

## k8s Pods

A _pod_ is the basic execution unit within k8s and can contain multiple containers in it. Within a pod, containers share networking and storage access. A pod is scheduled as a whole on one of the nodes in your cluster, meaning all containers in a pod will run on the same node. Multiple instances of a pod can be run across multiple cluster nodes.

By design, pods are ephemeral. They can be killed or moved to another node by the system, meaning any not explicitely persisted state can be lost. Typically, you won’t create individual pods directly; rather, you’ll create a deployment, which will create pods for you.

### Example

Create the following simple pod specification as `simple-nginx-pod.yaml` file:
```
apiVersion: v1
kind: Pod
metadata:
  name: simple-nginx-pod
spec:
  containers:
    - name: simple-nginx
      image: nginx
```

Then, deploy a new pod in your kubernetes cluster with the following command:
```
kubectl apply -f simple-nginx-pod.yam
```

Run `kubectl get pods` to see the pod was created. To verify that nginx is running:

```
kubectl exec -it simple-nginx-pod /bin/bash   # enter bash shell inside the container
# these commands are to be run inside the container
apt update                                    # prep the container
apt install curl -y                           # install curl  
curl localhost                                # verify nginx is active
```

k8s supports specifying the way resources will be observed and monitored, e.g. with _readinessProbes_ and _livenessProbes_. For resource as simple as pod, only _livenessProbes_ can be specifyed. `livenessProbe` can not be added to an active pod, another instance has to be created with this new definition, e.g save the following in `nginx-pod-liveness.yaml`:
```
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-liveness
spec:
  containers:
    - name: nginx-pod-liveness
      image: nginx
      livenessProbe:
        httpGet:
          path: /
          port: 80
        initialDelaySeconds: 15
        timeoutSeconds: 1
```

```
kubectl apply -f nginx-pod-liveness.yaml
kubectl get pods --watch
```

Now open up a second terminal, and do the following: to disturb nginx operations and to make our livenessProbe fail:
```
kubectl exec -it basic-pod /bin/bash
mv /usr/share/nginx/html/index.html /usr/share/nginx/html/index.html.backup 
```
In the first terminal, watching the pods in the cluster, it can be seen that Kubernetes has restarted our pod. In that restarted pod, we’ll have the original filesystem again (our move command will have been ‘undone’), and the `livenessProbe` will be succesfull again. If you want even more info on the restart of your pod, execute the following command:
```
kubectl describe pod/nginx-pod-liveness
```

---
